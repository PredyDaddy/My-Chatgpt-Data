{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23Z3pzV0ceh5"
   },
   "source": [
    "# ANLP Assignment (Autumn 2020)\n",
    "\n",
    "For assessment, you are expected to complete and submit this notebook file. When answers require code, you may import and use library functions (unless explicitly told otherwise). All of your own code should be included in the notebook rather than imported from elsewhere. Written answers should also be included in the notebook. You should insert as many extra cells as you want and change the type between code and markdown as appropriate.\n",
    "\n",
    "In order to avoid misconduct, you should not talk about the assignment questions with your peers. If you are not sure what a question is asking you to do or have any other questions, please ask me or one of the Teaching Assistants.\n",
    "\n",
    "Marking guidelines are provided as a separate document.\n",
    "\n",
    "The first few cells contain code to set-up the assignment and bring in some data. In order to provide unique datasets for analysis by different students, you must enter your candidate number in the following cell. Otherwise do not change the code in these cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5FRYFgGcdYH"
   },
   "outputs": [],
   "source": [
    "candidateno=11111119 #this MUST be updated to your candidate number so that you get a unique data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWe1DzridJTn"
   },
   "outputs": [],
   "source": [
    "#set up drives for resources.  Change the path as necessary\n",
    "\n",
    "from google.colab import drive\n",
    "#mount google drive\n",
    "drive.mount('/content/drive/')\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/NLE Notebooks/resources/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udnAzg7gdMmw"
   },
   "outputs": [],
   "source": [
    "#do not change the code in this cell\n",
    "#preliminary imports\n",
    "\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import zip_longest\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('wordnet_ic')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic as wn_ic\n",
    "brown_ic = wn_ic.ic(\"ic-brown.dat\")\n",
    "\n",
    "from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n",
    "from sussex_nltk.corpus_readers import ReutersCorpusReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cpSNtF3BFYK"
   },
   "source": [
    "# Question 1: Books vs DVDs\n",
    "\n",
    "In this question, you will be investigating NLP methods for distinguishing reviews written about books from reviews written about DVDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHBkzAccCVaZ"
   },
   "outputs": [],
   "source": [
    "#do not change the code in this cell\n",
    "def split_data(data, ratio=0.7): # when the second argument is not given, it defaults to 0.7\n",
    "    \"\"\"\n",
    "    Given corpus generator and ratio:\n",
    "     - partitions the corpus into training data and test data, where the proportion in train is ratio,\n",
    "\n",
    "    :param data: A corpus generator.\n",
    "    :param ratio: The proportion of training documents (default 0.7)\n",
    "    :return: a pair (tuple) of lists where the first element of the \n",
    "            pair is a list of the training data and the second is a list of the test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = list(data)  \n",
    "    n = len(data)  \n",
    "    train_indices = random.sample(range(n), int(n * ratio))          \n",
    "    test_indices = list(set(range(n)) - set(train_indices))    \n",
    "    train = [data[i] for i in train_indices]           \n",
    "    test = [data[i] for i in test_indices]             \n",
    "    return (train, test)                       \n",
    " \n",
    "\n",
    "def feature_extract(review):\n",
    "    \"\"\"\n",
    "    Generate a feature representation for a review\n",
    "    :param review: AmazonReview object\n",
    "    :return: dictionary of Boolean features\n",
    "    \"\"\"\n",
    "    return {word:True for word in review.words()}\n",
    "\n",
    "def get_training_test_data(categories=('book','dvd'),ratio=0.7,seed=candidateno):\n",
    "    \"\"\"\n",
    "    Get training and test data for a given pair of categories and ratio, pre-formatted for use with NB classifier\n",
    "    :param category: pair of categories of review corpus, two from [\"kitchen, \"dvd, \"book\", \"electronics\"]\n",
    "    :param ratio: proportion of data to use as training data\n",
    "    :return: pair of lists \n",
    "    \"\"\"\n",
    "    random.seed(candidateno)\n",
    "\n",
    "    train_data=[]\n",
    "    test_data=[]\n",
    "    for category in categories:\n",
    "      reader=AmazonReviewCorpusReader().category(category)    \n",
    "      train, test = split_data(reader.documents(),ratio=ratio)\n",
    "   \n",
    "      train_data+=[(feature_extract(review),category)for review in train]\n",
    "      test_data+=[(feature_extract(review),category)for review in test]\n",
    "    random.shuffle(train_data)\n",
    "    random.shuffle(test_data)\n",
    "\n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1N3LWwBYICPP"
   },
   "source": [
    "When you have run the cell below, your unique training and testing samples will be stored in `training_data` and `testing_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJLegkdPFUJA"
   },
   "outputs": [],
   "source": [
    "#do not change the code in this cell\n",
    "training_data,testing_data=get_training_test_data()\n",
    "print(\"The amount of training data is {}\".format(len(training_data)))\n",
    "print(\"The amount of testing data is {}\".format(len(testing_data)))\n",
    "print(\"The representation of a single data item is below\")\n",
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f6h0ON9I4NT"
   },
   "source": [
    "a) Use your training data to find\n",
    "i) the top 20 words which occur more frequently in book reviews than in dvd reviews\n",
    "ii) the top 20 words which occur more frequently in dvd reviews than book reviews\n",
    "Discuss what pre-processing techniques you have applied (or not applied) in answering this question, and why. [10 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wJ-ajVeFYjr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOCCcZDOKirm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWUMe5beKtgz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TApOQE6vND20"
   },
   "source": [
    "b) Design, build and test a word list classifier to classify reviews as being from the book domain or from the dvd domain.  Make sure you discuss i) how you decide the lengths and contents of the word lists and ii) accuracy, precision and recall of your final classifier.[15 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BThDMrcmODJy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCjcal0H7yQI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OyHbdCc97yYD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIS9UpmJNEAp"
   },
   "source": [
    "c) Compare the performance of your word list classifier with a Naive Bayes classifier (e.g., from NLTK).  Make sure you discuss the results. [10 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3YUiYKyrOSA0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5OkeMGN7zMY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ee8_NzW-7zQ8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGDXaVDqOSfY"
   },
   "source": [
    "d) Design and carry out an experiment into the impact of the amount of training data on each of these classifiers.  Make sure you describe design decisions in your experiment, include a graph of your results and discuss your conclusions. [15 marks] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgXiP6We70j0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ypOi_c-70p_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEFj6xr570sf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjuuKPQrAvSu"
   },
   "source": [
    "# Question 2: Distributional Semantics\n",
    "\n",
    "In this question, you will be investigating the *distributional hypothesis*: **words which appear in similar contexts tend to have similar meanings**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mfr4MFbOd5nx"
   },
   "source": [
    "We are going to be using the Reuters corpus of financial documents for this part of the assignment.  When you run the following cell you should see that it contains 1,113,359 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4kNJlEYd1qc"
   },
   "outputs": [],
   "source": [
    "#do not change the code in this cell\n",
    "rcr = ReutersCorpusReader().finance()\n",
    "rcr.enumerate_sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRApmpxneUqk"
   },
   "source": [
    "The following cell will take 2-5 minutes to run.  It will generate a unique-to-you sample of 200,000 sentences.  These sentences are tokenised and normalised for case and number for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSPRsF6deOAR"
   },
   "outputs": [],
   "source": [
    "#do not change the code in this cell\n",
    "def normalise(tokenlist):\n",
    "    tokenlist=[token.lower() for token in tokenlist]\n",
    "    tokenlist=[\"NUM\" if token.isdigit() else token for token in tokenlist]\n",
    "    tokenlist=[\"Nth\" if (token.endswith((\"nd\",\"st\",\"th\")) and token[:-2].isdigit()) else token for token in tokenlist]\n",
    "    tokenlist=[\"NUM\" if re.search(\"^[+-]?[0-9]+\\.[0-9]\",token) else token for token in tokenlist]\n",
    "    return tokenlist\n",
    "\n",
    "random.seed(candidateno)  \n",
    "samplesize=2000\n",
    "iterations =100\n",
    "sentences=[]\n",
    "for i in range(0,iterations):\n",
    "    sentences+=[normalise(sent) for sent in rcr.sample_sents(samplesize=samplesize)]\n",
    "    print(\"Completed {}%\".format(i))\n",
    "print(\"Completed 100%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDjqbB-WfhRg"
   },
   "source": [
    "`generate_features()` will used and explored below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FazdXtqJe1s-"
   },
   "outputs": [],
   "source": [
    "# do not change the code in this cell\n",
    "def generate_features(sentences,window=1):\n",
    "    mydict={}\n",
    "    for sentence in sentences:\n",
    "        for i,token in enumerate(sentence):\n",
    "            current=mydict.get(token,{})\n",
    "            features=sentence[max(0,i-window):i]+sentence[i+1:i+window+1]\n",
    "            for feature in features:\n",
    "                current[feature]=current.get(feature,0)+1\n",
    "            mydict[token]=current\n",
    "    return mydict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLv0yZx7f4ol"
   },
   "source": [
    "a) Run `generate_features(sentences[:5])`. With reference to the code and the specific examples, explain how the output was generated [5 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rnUZ_EogIW8"
   },
   "outputs": [],
   "source": [
    "generate_features(sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3YCEZUtgK2Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnfWmKkChEtG"
   },
   "source": [
    "b) Write code and **find** the 1000 most frequently occurring words that\n",
    "* are in your sample; AND\n",
    "* have at least one noun sense according to WordNet [5 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWa2AHk3hSL0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opO2PAMV4_uc"
   },
   "outputs": [],
   "source": [
    "# do not change the code in this cell.  It relates to part c)\n",
    "wordpair=(\"house\",\"garden\")\n",
    "concept_1=wn.synsets(wordpair[0])[0]\n",
    "concept_2=wn.synsets(wordpair[1])[0]\n",
    "print(\"Path similarity between 1st sense of {} and 1st sense of {} is {}\".format(wordpair[0],wordpair[1],wn.path_similarity(concept_1,concept_2)))\n",
    "print(\"Resnik similarity between 1st sense of {} and 1st sense of {} is {}\".format(wordpair[0],wordpair[1],wn.res_similarity(concept_1,concept_2, brown_ic)))\n",
    "print(\"Lin similarity between 1st sense of {} and 1st sense of {} is {}\".format(wordpair[0],wordpair[1],wn.lin_similarity(concept_1,concept_2, brown_ic)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezzW3xqpszB2"
   },
   "source": [
    "c) \n",
    "i) The code above outputs the path similarity score, the Resnik similarity score and the Lin similarity score for a pair of concepts in WordNet.  Explain what each of these numbers means.\n",
    "\n",
    "ii) For every possible pair of words identified in Q2, determine the semantic similarity of the pair according to WordNet.  Make sure you justify your choice of semantic similarity measure and explain and justify the strategy used for words with multiple senses.\n",
    "\n",
    "iii) Identify the 10 most similar words (according to WordNet) to the most frequent word in the corpus [15 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBJ2fpwyvLsS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BxDUsFkhSzD"
   },
   "source": [
    "d)\n",
    "i) Write code to construct distributional vector representations of words in the corpus with a parameter to specify context size.  Explain how you calculate the value of association between each word and each context feature.\n",
    "\n",
    "ii) Use your code to construct representations of the 1000 words identified in Q2 with a window size of 1 and thus determine the 10 words which are distributionally most similar to the most frequent word in the corpus. [10 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k5mZKdJdrY4Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uo009tKWrZoB"
   },
   "source": [
    "e) Plan and carry out an investigation into the correlation between semantic similarity according to WordNet and distributional similarity with different context window sizes. You should make sure that you include a graph of how correlation varies with context window size and that you discuss your results. [15 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JBymlTzsviaj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1084,
     "status": "ok",
     "timestamp": 1604406679445,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "",
      "userId": "13844540934373660130"
     },
     "user_tz": 0
    },
    "id": "34rdlS_iPov6",
    "outputId": "a00096d3-eeb6-41ed-ee4a-fd23a2869cdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission length is 0\n"
     ]
    }
   ],
   "source": [
    "##This code will word count all of the markdown cells in the notebook saved at filepath\n",
    "##Running it before providing any answers shows that the questions have a word count of 388\n",
    "\n",
    "import io\n",
    "from nbformat import current\n",
    "\n",
    "filepath=\"/content/drive/My Drive/NLE Notebooks/assessment/ANLPassignment.ipynb\"\n",
    "question_count=754\n",
    "\n",
    "with io.open(filepath, 'r', encoding='utf-8') as f:\n",
    "    nb = current.read(f, 'json')\n",
    "\n",
    "word_count = 0\n",
    "for cell in nb.worksheets[0].cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print(\"Submission length is {}\".format(word_count-question_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdWwy0O4CmTn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMNB/Sk64tomxXKneqMCsPu",
   "collapsed_sections": [],
   "name": "ANLPassignment.ipynb",
   "provenance": [
    {
     "file_id": "1LcBJXs4cnjeiOeF-R4VgxelPPUH_Wbz6",
     "timestamp": 1604405292575
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
